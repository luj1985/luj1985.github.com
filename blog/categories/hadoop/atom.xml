<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hadoop | Lu Jun's blog]]></title>
  <link href="http://luj1985.github.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://luj1985.github.com/"/>
  <updated>2012-11-10T20:30:44+08:00</updated>
  <id>http://luj1985.github.com/</id>
  <author>
    <name><![CDATA[Lu Jun]]></name>
    <email><![CDATA[luj1985@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Construct Hadoop Environment<br />Part 5 Install Hadoop]]></title>
    <link href="http://luj1985.github.com/blog/2012/03/25/construct-hadoop-environment-part-5/"/>
    <updated>2012-03-25T14:49:00+08:00</updated>
    <id>http://luj1985.github.com/blog/2012/03/25/construct-hadoop-environment-part-5</id>
    <content type="html"><![CDATA[<p>Finally got here to start real work :)</p>

<h1>Create VM Template</h1>

<h2>Create Linux user to run Hadoop</h2>

<p>Here <em>gentoo</em> is the machine name of my VM, and the fully qualified domain name of this VM is <em>gentoo.hcluster.org</em>, and the IP address is <em>192.168.2.103</em>
<code>
mkdir /hadoop
useradd -d /hadoop -s /bin/bash -g users -G wheel hadoop
passwd hadoop
chown -R hadoop.users /hadoop
</code></p>

<h2>Create SSH public key</h2>

<p><code>
ssh hadoop@gentoo.hcluster.org
ssh-keygen
cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys
</code>
This VM is used as template, if it can connect to itself, it can connect to any VM which derived from it.
Here is an easy way to copy the SSH public key.
<code>
ssh-copy-id -i ~/.ssh/id_dsa.pub hadoop@gentoo.hcluster.org
</code></p>

<p>Edit <em>~/.ssh/config</em> to prevent host key checking
<code>
Host *.hcluster.org
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
</code>
Now it could be able to connect any host which domain is <em>hcluster.org</em> without password checking.</p>

<h2>Install</h2>

<p>Download <a href="http://hadoop.apache.org/">Apache Hadoop</a>, currently the latest version is 1.0.1
<code>
wget http://mirror.bjtu.edu.cn/apache/hadoop/common/stable/hadoop-1.0.1.tar.gz
tar xzf hadoop-1.0.1.tar.gz
ln -s hadoop-1.0.1 hadoop
</code></p>

<p>Edit <em>~/.bash_profile</em> to update environment variables
<code>
export HADOOP_COMMON_HOME=/hadoop/hadoop
export PATH=$HADOOP_COMMON_HOME/bin:$PATH
</code></p>

<p>Edit <em>$HADOOP_COMMON_HOME/conf/hadoop-env.sh</em>, add <em>JAVA_HOME</em> environment variable setting.
<code>
export JAVA_HOME=`java-config --jdk-home`
</code></p>

<h1>Configure Apache Hadoop environment</h1>

<p>Here is my environment</p>

<h2>```</h2>

<h2>|       Domain Name       |   IP Address  |    MAC Address    |</h2>

<p>| jobtracker.hcluster.org | 192.168.2.104 | 00:00:00:00:00:04 |
| namenode1.hcluster.org  | 192.168.2.105 | 00:00:00:00:00:05 |
| namenode2.hcluster.org  | 192.168.2.106 | 00:00:00:00:00:06 |
| datanode1.hcluster.org  | 192.168.2.107 | 00:00:00:00:00:07 |
| datanode2.hcluster.org  | 192.168.2.108 | 00:00:00:00:00:08 |
| datanode3.hcluster.org  | 192.168.2.109 | 00:00:00:00:00:09 |
| datanode4.hcluster.org  | 192.168.2.110 | 00:00:00:00:00:10 |</p>

<h2>| datanode5.hcluster.org  | 192.168.2.111 | 00:00:00:00:00:11 |</h2>

<p>```</p>

<h2><em>$HADOOP_COMMON_HOME/conf/core-site.xml</em></h2>

<p>Hadoop common properties
```
&lt;?xml version="1.0"?>
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></p>

<!-- Put site-specific property overrides in this file. -->


<p><configuration></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;fs.default.name&lt;/name&gt;
    &lt;value&gt;hdfs://namenode1.hcluster.org&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p></configuration>
```</p>

<h2><em>$HADOOP_COMMON_HOME/conf/hdfs-site.xml</em></h2>

<p>HDFS properties
```
&lt;?xml version="1.0"?>
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></p>

<!-- Put site-specific property overrides in this file. -->


<p><configuration></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;2&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p></configuration>
```</p>

<h2><em>$HADOOP_COMMON_HOME/conf/mapred-site.xml</em></h2>

<p>MapReduce properties
```
&lt;?xml version="1.0"?>
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></p>

<!-- Put site-specific property overrides in this file. -->


<p><configuration></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;mapred.job.tracker&lt;/name&gt;
    &lt;value&gt;jobtracker.hcluster.org:9000&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p></configuration>
```
Must provide port here, seems Hadoop doesn't define the default port for job tracker.</p>

<h2><em>$HADOOP_COMMON_HOME/conf/slaves</em></h2>

<p><code>
datanode1.hcluster.org
datanode2.hcluster.org
datanode3.hcluster.org
datanode4.hcluster.org
datanode5.hcluster.org
</code></p>

<h2><em>$HADOOP_COMMON_HOME/conf/masters</em></h2>

<p>Secondary name node
<code>
namenode2.hcluster.org
</code></p>

<h1>Clone VM</h1>

<p>Remove udev rule file under <em>/etc/udev/rules.d/</em></p>

<h2>70-persistent-net.rules</h2>

<p>When system start it will read udev rules to create device file.
The device file <code>eth0</code> was bind to its MAC address.
That is to say when MAC address changed, the device file name will change too
(Treat as another network card). This makes <em>/etc/init.d/net.eth0</em> invalid.</p>

<p>These rule files can be generated automatically next reboot. So remove this rule file and reboot can adapt MAC changes.</p>

<p>Shutdown this VM, and create several clones to construct Apache Hadoop Cluster</p>

<h2>Change the hostname</h2>

<p>Here the VMs are cloned from one template, all the VM hostnames are the same.</p>

<p>During Hadoop execution, job tracker will <code>map</code> the task to every task tracker node, when <code>map</code> finished it will do the <code>reduce</code>, task trackers trying to copy
data to <code>reduce</code> machine, but here Hadoop seems use <code>hostname</code> to connect the machine.<br/>
Host Name is different from Domain Name, in order to make Hadoop work, I have to make them same</p>

<p>Edit <em>/etc/conf.d/hostname</em> for every node, then restart hostname service
<code>
ssh root@jobtracker.hcluster.org "echo hostname=\\\"jobtracker\\\" &gt; /etc/conf.d/hostname;/etc/init.d/hostname restart"
ssh root@namenode1.hcluster.org "echo hostname=\\\"namenode1\\\" &gt; /etc/conf.d/hostname;/etc/init.d/hostname restart"
ssh root@namenode2.hcluster.org "echo hostname=\\\"namenode2\\\" &gt; /etc/conf.d/hostname;/etc/init.d/hostname restart"
ssh root@datanode1.hcluster.org "echo hostname=\\\"datanode1\\\" &gt; /etc/conf.d/hostname;/etc/init.d/hostname restart"
ssh root@datanode2.hcluster.org "echo hostname=\\\"datanode2\\\" &gt; /etc/conf.d/hostname;/etc/init.d/hostname restart"
ssh root@datanode3.hcluster.org "echo hostname=\\\"datanode3\\\" &gt; /etc/conf.d/hostname;/etc/init.d/hostname restart"
ssh root@datanode4.hcluster.org "echo hostname=\\\"datanode4\\\" &gt; /etc/conf.d/hostname;/etc/init.d/hostname restart"
ssh root@datanode5.hcluster.org "echo hostname=\\\"datanode5\\\" &gt; /etc/conf.d/hostname;/etc/init.d/hostname restart"
</code></p>

<h1>Start Apache Hadoop cluster</h1>

<p>Log into <code>namenode1</code> with <code>hadoop</code> account
<code>
ssh hadoop@namenode1.hcluster.org
</code>
Format namenode and start cluster
<code>
hadoop namenode -format
start-all.sh
</code>
after cluster started, view logs to find whether cluster was started successfully.
Here I found job tracker has errors
```
2012-04-07 13:23:26,317 FATAL org.apache.hadoop.mapred.JobTracker: java.net.BindException: Problem binding to jobtracker.hcluster.org/192.168.2.104:9000 : Cannot assign requested address</p>

<pre><code>at org.apache.hadoop.ipc.Server.bind(Server.java:227)
at org.apache.hadoop.ipc.Server$Listener.&lt;init&gt;(Server.java:301)
at org.apache.hadoop.ipc.Server.&lt;init&gt;(Server.java:1483)
at org.apache.hadoop.ipc.RPC$Server.&lt;init&gt;(RPC.java:545)
at org.apache.hadoop.ipc.RPC.getServer(RPC.java:506)
at org.apache.hadoop.mapred.JobTracker.&lt;init&gt;(JobTracker.java:2306)
at org.apache.hadoop.mapred.JobTracker.&lt;init&gt;(JobTracker.java:2192)
at org.apache.hadoop.mapred.JobTracker.&lt;init&gt;(JobTracker.java:2186)
at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:300)
at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:291)
at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:4978)
</code></pre>

<p>Caused by: java.net.BindException: Cannot assign requested address</p>

<pre><code>at sun.nio.ch.Net.bind(Native Method)
at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:137)
at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:77)
at org.apache.hadoop.ipc.Server.bind(Server.java:225)
... 10 more
</code></pre>

<p><code>
I don't know why this happened, but I could start job tracker on machine `jobtracker.hcluster.org`
</code>
ssh hadoop@jobtracker.hcluster.org
hadoop-daemon.sh start jobtracker
```</p>

<h1>Test Cluster</h1>

<p>Follow the offical Apache Hadoop examples
<code>
hadoop fs -put conf input
hadoop jar hadoop-examples-1.0.1.jar grep input output 'dfs[a-z.]+'
</code></p>

<h1>Postscript</h1>

<p>Though Hadoop could runs on this "Cluster", but the performance is very low.
I think I'd better use Hadoop Pseudo-Distributed mode instead...</p>
]]></content>
  </entry>
  
</feed>
